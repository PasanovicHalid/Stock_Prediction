{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c2adffd-0071-462e-84a4-257385fb7684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\Desktop\\fax\\master\\siap\\Stock_Prediction\\env\\lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\Desktop\\fax\\master\\siap\\Stock_Prediction\\env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\Desktop\\fax\\master\\siap\\Stock_Prediction\\env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "13/13 [==============================] - 20s 289ms/step - loss: 0.0350 - mse: 0.0350 - mae: 0.1471 - val_loss: 0.0187 - val_mse: 0.0187 - val_mae: 0.1065\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0928 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0824\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0739 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0641\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0648 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0615\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0622 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0619\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0586 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0550\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0568 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0501\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0564 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0475\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0524 - val_loss: 0.0031 - val_mse: 0.0031 - val_mae: 0.0457\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 1s 48ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0492 - val_loss: 0.0029 - val_mse: 0.0029 - val_mae: 0.0451\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 36ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0497 - val_loss: 0.0029 - val_mse: 0.0029 - val_mae: 0.0453\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0446 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0479\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0441 - val_loss: 0.0029 - val_mse: 0.0029 - val_mae: 0.0459\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 36ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0438 - val_loss: 0.0019 - val_mse: 0.0019 - val_mae: 0.0368\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0412 - val_loss: 0.0020 - val_mse: 0.0020 - val_mae: 0.0379\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0424 - val_loss: 0.0021 - val_mse: 0.0021 - val_mae: 0.0382\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0021 - mse: 0.0021 - mae: 0.0367 - val_loss: 0.0017 - val_mse: 0.0017 - val_mae: 0.0334\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 38ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0376 - val_loss: 0.0021 - val_mse: 0.0021 - val_mae: 0.0371\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 35ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0402 - val_loss: 0.0031 - val_mse: 0.0031 - val_mae: 0.0429\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0397 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0299\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'algorithms/cnn-lstm/checkpoints/AAPL/cnn-lstm-1.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 152\u001b[0m\n\u001b[0;32m    149\u001b[0m     train_model_for_stocks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 149\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m--> 149\u001b[0m     \u001b[43mtrain_model_for_stocks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAAPL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 144\u001b[0m, in \u001b[0;36mtrain_model_for_stocks\u001b[1;34m(company_name)\u001b[0m\n\u001b[0;32m    142\u001b[0m train_X_values,test_X_values,train_Y_values,test_Y_values \u001b[38;5;241m=\u001b[39m get_train_and_test_values(data)\n\u001b[0;32m    143\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model()\n\u001b[1;32m--> 144\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_X_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_X_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_Y_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_Y_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[1;32mIn[10], line 102\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(company_name, model, train_X_values, test_X_values, train_Y_values, test_Y_values)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_X_values)):\n\u001b[0;32m    101\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(train_X_values[i], train_Y_values[i], validation_data\u001b[38;5;241m=\u001b[39m(test_X_values[i],test_Y_values[i]), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malgorithms/cnn-lstm/checkpoints/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mcompany_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/cnn-lstm-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\Desktop\\fax\\master\\siap\\Stock_Prediction\\env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\zipfile.py:1248\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(file, filemode)\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'algorithms/cnn-lstm/checkpoints/AAPL/cnn-lstm-1.keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dropout,Conv1D, MaxPooling1D, Flatten,TimeDistributed,Bidirectional\n",
    "import math\n",
    "import datetime as dt\n",
    "from datetime import datetime    \n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import MaxPooling1D, Flatten\n",
    "from tensorflow.keras.regularizers import L1, L2\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "\n",
    "def import_data_from_csv(path: str):\n",
    "    \"\"\"\n",
    "    Imports data from a csv file and returns a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def reshape_data(df):\n",
    "    X = []\n",
    "    Y = []\n",
    "    window_size=100\n",
    "    for i in range(1 , len(df) - window_size -1 , 1):\n",
    "        first = df.iloc[i,4]\n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        for j in range(window_size):\n",
    "            temp.append((df.iloc[i + j, 4] - first) / first)\n",
    "        temp2.append((df.iloc[i + window_size, 4] - first) / first)\n",
    "        X.append(np.array(temp).reshape(100, 1))\n",
    "        Y.append(np.array(temp2).reshape(1, 1))\n",
    "    return X,Y\n",
    "\n",
    "def get_train_and_test_values(df):\n",
    "    X,Y = reshape_data(df)\n",
    "    train_X_values,test_X_values,train_Y_values,test_Y_values = [],[],[],[]\n",
    "    \n",
    "    num_splits = 5\n",
    "    split_size = len(X) / num_splits\n",
    "    \n",
    "    X_splits = [X[int(i * split_size):int((i + 1) * split_size)] for i in range(num_splits)]\n",
    "    Y_splits = [Y[int(i * split_size):int((i + 1) * split_size)] for i in range(num_splits)]\n",
    "    \n",
    "        \n",
    "    for i in range(num_splits):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X_splits[i], Y_splits[i], test_size=0.2, shuffle=True)\n",
    "        train_X = np.array(x_train)\n",
    "        test_X = np.array(x_test)\n",
    "        train_Y = np.array(y_train)\n",
    "        test_Y = np.array(y_test)\n",
    "        \n",
    "        train_X = train_X.reshape(train_X.shape[0],1,100,1)\n",
    "        test_X = test_X.reshape(test_X.shape[0],1,100,1)\n",
    "        \n",
    "        train_X_values.append(train_X)\n",
    "        test_X_values.append(test_X)\n",
    "        train_Y_values.append(train_Y)\n",
    "        test_Y_values.append(test_Y)\n",
    "\n",
    "    return train_X_values,test_X_values,train_Y_values,test_Y_values\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Creating the Neural Network model here...\n",
    "    # CNN layers\n",
    "    model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu', input_shape=(None, 100, 1))))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    # LSTM layers\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #Final layers\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(company_name,model,train_X_values,test_X_values,train_Y_values,test_Y_values):\n",
    "    for i in range(len(train_X_values)):\n",
    "        model.fit(train_X_values[i], train_Y_values[i], validation_data=(test_X_values[i],test_Y_values[i]), epochs=20,batch_size=40, verbose=1, shuffle =True)\n",
    "        model.save(\"algorithms/cnn-lstm/checkpoints/\"+company_name+\"/cnn-lstm-\"+str(i+1)+\".keras\")\n",
    "    return model\n",
    "\n",
    "def get_test_X_and_test_Y_data(data2):\n",
    "    data2.dropna(inplace=True)\n",
    "    data2.reset_index(drop=True, inplace=True)\n",
    "    df2 = data2.drop('Date', axis=1)\n",
    "    \n",
    "    X,Y = reshape_data(df2)\n",
    "    \n",
    "    test_X = np.array(X)\n",
    "    test_Y = np.array(Y)\n",
    "    \n",
    "    test_X = test_X.reshape(test_X.shape[0],1,100,1)\n",
    "\n",
    "    return test_X,test_Y\n",
    "\n",
    "def evaluate_model(model,test_X,test_Y):\n",
    "    model.evaluate(test_X, test_Y)\n",
    "    predicted  = model.predict(test_X)\n",
    "    test_label = test_Y.reshape(-1,1)\n",
    "    predicted = np.array(predicted[:,0]).reshape(-1,1)\n",
    "    len_t = len(test_X)\n",
    "    print(len_t)\n",
    "    for j in range(len_t):\n",
    "        temp = data2.iloc[j,3]\n",
    "        test_label[j - len_t] = test_label[j - len_t] * temp + temp\n",
    "        predicted[j - len_t] = predicted[j - len_t] * temp + temp\n",
    "    plt.plot(test_label, color = 'red', label = 'Real Stock Price')\n",
    "    plt.plot(predicted, color = 'green', label = 'Predicted  Stock Price')\n",
    "    plt.title(' Stock Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(' Stock Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_model_for_stocks(company_name):\n",
    "    file_name = \"data\\\\processed\\\\\"\n",
    "    file_name += company_name+'.csv'\n",
    "    data = import_data_from_csv(file_name)\n",
    "    train_X_values,test_X_values,train_Y_values,test_Y_values = get_train_and_test_values(data)\n",
    "    model = create_model()\n",
    "    model = train_model(company_name,model,train_X_values,test_X_values,train_Y_values,test_Y_values)\n",
    "    return model\n",
    "    \n",
    "\n",
    "def main():\n",
    "    train_model_for_stocks(\"AAPL\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fd0f6-254c-47bc-97bc-6265825e9c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
