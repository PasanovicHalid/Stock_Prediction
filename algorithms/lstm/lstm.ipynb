{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">LSTM Model for Stock Prediction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import of Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from re import X\n",
    "\n",
    "import keras;\n",
    "from kerastuner import HyperModel, RandomSearch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Functions for Loading preprocessed data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_from_csv(path: str):\n",
    "    \"\"\"\n",
    "    Imports data from a csv file and returns a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Functions for preparing data for LSTM model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_required_for_training(df: pd.DataFrame, features: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features required for training.\n",
    "    \"\"\"\n",
    "    return df[features]\n",
    "\n",
    "\n",
    "def split_data_into_training_and_test_sets(df: pd.DataFrame, window_start: int, window_end: int, test_size: float, columns: list[str]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Normalizes the entire data set, then splits it into training and test sets based on the given ratio.\n",
    "    \"\"\"\n",
    "    data_count = window_end - window_start\n",
    "    test_data_count = int(data_count * test_size)\n",
    "\n",
    "    end_of_training_data = window_start + test_data_count\n",
    "\n",
    "    df, max_values = normalize_data(df, columns, window_start, window_end)\n",
    "\n",
    "    return df.iloc[window_start : end_of_training_data], df.iloc[end_of_training_data : window_end], max_values\n",
    "\n",
    "\n",
    "def normalize_data(df: pd.DataFrame, columns: list[str], start: int, end: int):\n",
    "    \"\"\"\n",
    "    Normalizes the data in the given segment.\n",
    "    \"\"\"\n",
    "    max_values = {}\n",
    "    for column in columns:\n",
    "        max_value = df.iloc[start:end][column].max()\n",
    "        df.loc[start:end, column] = df.loc[start:end, column] / max_value\n",
    "        max_values[column] = max_value\n",
    "\n",
    "    return df, max_values\n",
    "\n",
    "\n",
    "def denormalize_data(data_array: np.ndarray, columns: list[str], max_values: dict):\n",
    "    \"\"\"\n",
    "    Denormalizes the data in the given segment.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        data_array[:] = data_array[:] * max_values[column]\n",
    "\n",
    "    return data_array\n",
    "\n",
    "\n",
    "def create_dataset(dataset: pd.DataFrame, time_step = 1) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Creates a dataset.\n",
    "    \"\"\"\n",
    "    dataX,dataY = [],[]\n",
    "\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        dataX.append(dataset.iloc[i:(i+time_step), 0])\n",
    "        dataY.append(dataset.iloc[i + time_step, 0])\n",
    "\n",
    "    return np.array(dataX),np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function for Building the LSTM model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds the LSTM model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(75, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(LSTM(75, return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(LSTM(75, return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(LSTM(75))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(512, activation=\"relu\")) \n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(1, activation=\"relu\"))\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"mae\", 'mape'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function for Hyperparameter optimization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=hp.Int('units', min_value=25, max_value=75, step=10), \n",
    "                    return_sequences=True, \n",
    "                    input_shape=self.input_shape))\n",
    "        model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        for i in range(hp.Int('layers', 1, 3)):\n",
    "            model.add(LSTM(units=hp.Int('units_' + str(i), min_value=25, max_value=75, step=10), \n",
    "                        return_sequences=True))\n",
    "            model.add(Dropout(hp.Float('dropout_' + str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(LSTM(units=hp.Int('units_final', min_value=32, max_value=512, step=32)))\n",
    "        model.add(Dropout(hp.Float('dropout_final', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        for i in range(hp.Int('dense_layers', 1, 3)):\n",
    "            model.add(Dense(hp.Int('dense_units_' + str(i), min_value=128, max_value=512, step=64), \n",
    "                            activation=hp.Choice('dense_activation_' + str(i), values=['relu', 'tanh', 'sigmoid'])))\n",
    "            model.add(Dropout(hp.Float('dense_dropout_' + str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(1, activation=\"relu\"))\n",
    "\n",
    "        model.compile(loss=\"mse\", \n",
    "                    optimizer=keras.optimizers.Adam(\n",
    "                        hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "                    metrics=[\"mae\", 'mape'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Functions for representing data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_on_whole_dataset(model, data, max_values, time_step, forecast_size):\n",
    "    \"\"\"\n",
    "    Forecasts the data.\n",
    "    \"\"\"\n",
    "    forecast = []\n",
    "    for i in range(forecast_size):\n",
    "        forecast.append(model.predict(data))\n",
    "        data = np.append(data, forecast[-1])\n",
    "        data = np.delete(data, 0)\n",
    "        data = data.reshape(1, time_step, 1)\n",
    "    forecast = np.array(forecast)\n",
    "    forecast = forecast.reshape(forecast_size, 1)\n",
    "    forecast = denormalize_data(forecast, [\"Close\"], max_values)\n",
    "    return forecast\n",
    "\n",
    "def plot_data(loaded_data, title: str, xlabel: str, ylabel: str, datacolumn: str):\n",
    "    \"\"\"\n",
    "    Plots the data from the dataframe.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.title(title)\n",
    "    plt.plot(loaded_data[datacolumn])\n",
    "    plt.xlabel(xlabel, fontsize=18)\n",
    "    plt.ylabel(ylabel, fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_data_list(loaded_data_list, title: str, xlabel: str, ylabel: str, datacolumns: list):\n",
    "    \"\"\"\n",
    "    Plots the data from the dataframe.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.title(title)\n",
    "    for loaded_data, datacolumn in zip(loaded_data_list, datacolumns):\n",
    "        plt.plot(loaded_data, label=datacolumn)\n",
    "    plt.xlabel(xlabel, fontsize=18)\n",
    "    plt.ylabel(ylabel, fontsize=18)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_all_data(all_actual, all_predictions, title, xlabel, ylabel, legend_pred, legend_act):\n",
    "    colors = ['red', 'yellow', 'pink', 'gray', 'orange']  # List of colors to cycle through\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Concatenate all actual data and plot it in one color\n",
    "    all_actual_concat = np.concatenate(all_actual)\n",
    "    plt.plot(all_actual_concat, color='lightblue', label=legend_act)\n",
    "\n",
    "    # Plot each predicted segment with a different color\n",
    "    start = 0\n",
    "    for i, predictions in enumerate(all_predictions):\n",
    "        end = start + len(predictions)\n",
    "        x = range(start, end)\n",
    "        color = colors[i % len(colors)]  # Cycle through colors\n",
    "        plt.plot(x, predictions, color=color, label=legend_pred + ' ' + str(i+1))\n",
    "        start = end\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function for fiting the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Y\n",
    "\n",
    "\n",
    "def lstm_prediction(file: str):\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    file_name_without_extension = file_name.split(\".\")[0]\n",
    "\n",
    "    loaded_data = import_data_from_csv(file)\n",
    "    extracted_data = extract_features_required_for_training(loaded_data, [\"Adj Close\"])\n",
    "\n",
    "    time_step = 5\n",
    "    number_of_intervals = 5\n",
    "    percentage_of_test_interval_data = 0.8\n",
    "\n",
    "    interval_size = int(extracted_data.shape[0] / number_of_intervals)\n",
    "\n",
    "    X_training_data, Y_training_data = [], []\n",
    "    X_test_data, Y_test_data = [], []\n",
    "    X_all_data, Y_all_data = [], []\n",
    "    max_values_list = []\n",
    "\n",
    "    for i in range(number_of_intervals):\n",
    "        start = i * interval_size\n",
    "        end = start + interval_size\n",
    "\n",
    "        if(i == number_of_intervals - 1):\n",
    "            end = extracted_data.shape[0]\n",
    "\n",
    "        training_data_interval, test_data_interval, max_values = split_data_into_training_and_test_sets(extracted_data, start, end, percentage_of_test_interval_data, [\"Adj Close\"])\n",
    "        \n",
    "        X_train, Y_train =  create_dataset(training_data_interval, time_step)\n",
    "        X_test, Y_test =  create_dataset(test_data_interval, time_step)\n",
    "        X_all = np.concatenate((X_train, X_test))\n",
    "        Y_all = np.concatenate((Y_train, Y_test))\n",
    "\n",
    "        if(i != 0):\n",
    "            X_train = np.concatenate((X_test_data[i - 1], X_train))\n",
    "            Y_train = np.concatenate((Y_test_data[i - 1], Y_train))\n",
    "\n",
    "        X_training_data.append(X_train)\n",
    "        Y_training_data.append(Y_train)\n",
    "\n",
    "        X_test_data.append(X_test)\n",
    "        Y_test_data.append(Y_test)\n",
    "\n",
    "        X_all_data.append(X_all)\n",
    "        Y_all_data.append(Y_all)\n",
    "\n",
    "        max_values_list.append(max_values)\n",
    "\n",
    "    X_train_shape = (X_training_data[0].shape[1], 1)\n",
    "    #lstm_model = build_model(X_train_shape)\n",
    "\n",
    "    predictions_list = []\n",
    "    actuals_list = []\n",
    "\n",
    "    all_actual_for_intervals_list = []\n",
    "    all_predictions_for_intervals_list = []\n",
    "\n",
    "    for i in range(number_of_intervals):\n",
    "        X_training_data_interval = X_training_data[i]\n",
    "        Y_training_data_interval = Y_training_data[i]\n",
    "        X_test_data_interval = X_test_data[i]\n",
    "        Y_test_data_interval = Y_test_data[i]\n",
    "        X_all_data_interval = X_all_data[i]\n",
    "        Y_all_data_interval = Y_all_data[i]\n",
    "        \n",
    "        file_path = \"checkpoints/\" + file_name_without_extension + \"/lstm-\" + str(i + 1) + \".keras\"\n",
    "\n",
    "        if(os.path.exists(file_path)):\n",
    "            lstm_model = load_model(file_path)\n",
    "            lstm_model.summary()\n",
    "\n",
    "        else:\n",
    "            file_path_previos = \"checkpoints/\" + file_name_without_extension + \"/lstm-\" + str(i) + \".keras\"\n",
    "\n",
    "            if(os.path.exists(file_path_previos)):\n",
    "                lstm_model = load_model(file_path_previos)\n",
    "\n",
    "            X_training_data_interval = np.reshape(X_training_data_interval, (X_training_data_interval.shape[0], X_training_data_interval.shape[1], 1))\n",
    "\n",
    "            ###Before hyperparameter tuning\n",
    "            #t_hist = lstm_model.fit(X_training_data_interval, Y_training_data_interval, batch_size = 15, epochs = 50)\n",
    "            \n",
    "            ###Hyperparameter tuning\n",
    "            hypermodel = LSTMHyperModel(input_shape=X_train_shape)\n",
    "            tuner = RandomSearch(\n",
    "                hypermodel,\n",
    "                objective='val_loss',\n",
    "                max_trials=20,\n",
    "                executions_per_trial=2,\n",
    "                directory='tuning_results',\n",
    "                project_name='lstm'+file_name_without_extension+str(i))\n",
    "\n",
    "            tuner.search(X_training_data_interval, Y_training_data_interval,\n",
    "                        epochs=20,\n",
    "                        validation_data=(X_test_data_interval, Y_test_data_interval))\n",
    "\n",
    "            best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "            lstm_model = tuner.hypermodel.build(best_hps)\n",
    "            t_hist = lstm_model.fit(X_training_data_interval, Y_training_data_interval, batch_size = 15, epochs = 50)\n",
    "            \n",
    "            pd.DataFrame(t_hist.history).plot(figsize=(8,5))\n",
    "            plt.show()\n",
    "\n",
    "            lstm_model.save(file_path)\n",
    "        \n",
    "        X_test_data_interval = np.reshape(X_test_data_interval, (X_test_data_interval.shape[0], X_test_data_interval.shape[1] ,1))\n",
    "        predictions = lstm_model.predict(X_test_data_interval)\n",
    "\n",
    "        denormalized_predictions = denormalize_data(predictions, [\"Adj Close\"], max_values_list[i])\n",
    "        denormalized_actuals = denormalize_data(Y_test_data_interval, [\"Adj Close\"], max_values_list[i])\n",
    "        plot_data_list([denormalized_predictions.flatten(), denormalized_actuals.flatten()], 'Predictions vs Actual for ' + str(i+1) + '. iteration of ' + file_name_without_extension, 'Date', 'Adj Close Price USD ($)', ['Predictions', 'Actual'])\n",
    "\n",
    "        rmse = np.sqrt(np.mean(((predictions - Y_test_data_interval) ** 2)))\n",
    "        mae = np.mean(np.abs(predictions - Y_test_data_interval))\n",
    "        print(\"Value of RSME for interval \" + str(i + 1) + \" is: \" + str(rmse))\n",
    "        print(\"Value of MAE for interval \" + str(i + 1) + \" is: \" + str(mae))\n",
    "\n",
    "        X_all_data_interval = np.reshape(X_all_data_interval, (X_all_data_interval.shape[0], X_all_data_interval.shape[1] ,1))\n",
    "        all_predictions_for_interval = lstm_model.predict(X_all_data_interval)\n",
    "\n",
    "        denormalized_predictions_for_interval = denormalize_data(all_predictions_for_interval, [\"Adj Close\"], max_values_list[i])\n",
    "        denormalized_actuals_for_interval = denormalize_data(Y_all_data_interval, [\"Adj Close\"], max_values_list[i])\n",
    "\n",
    "        all_actual_for_intervals_list.append(denormalized_actuals_for_interval)\n",
    "        all_predictions_for_intervals_list.append(denormalized_predictions_for_interval)\n",
    "\n",
    "        predictions_list = np.concatenate((predictions_list, denormalized_predictions.flatten()))\n",
    "        actuals_list = np.concatenate((actuals_list, denormalized_actuals.flatten()))\n",
    "\n",
    "\n",
    "    plot_data_list([predictions_list, actuals_list], 'Predictions vs Actual for ' + file_name_without_extension, 'Date', 'Adj Close Price USD ($)', ['Predictions', 'Actual'])\n",
    "    print(\"NOTE: The big jumps in the values happen because the intervals have 80% of the training data between them.\")\n",
    "\n",
    "    plot_all_data(all_actual_for_intervals_list, all_predictions_for_intervals_list, 'Predictions vs Actual for ' + file_name_without_extension, 'Date', 'Adj Close Price USD ($)', 'Predictions', 'Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Main function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_71200\\2236464972.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[start:end, column] = df.loc[start:end, column] / max_value\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.src.backend' has no attribute 'floatx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/processed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mlstm_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../data/processed/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 71\u001b[0m, in \u001b[0;36mlstm_prediction\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     68\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file_name_without_extension \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/lstm-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path)):\n\u001b[1;32m---> 71\u001b[0m     lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     lstm_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mg:\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:254\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         )\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
      "File \u001b[1;32mg:\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:281\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    278\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mg:\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:246\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[1;32m--> 246\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[1;32mg:\\Python311\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m safe_mode_scope \u001b[38;5;241m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m--> 728\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m build_config:\n",
      "File \u001b[1;32mg:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\sequential.py:463\u001b[0m, in \u001b[0;36mSequential.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    461\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     layer_configs \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 463\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_config \u001b[38;5;129;01min\u001b[39;00m layer_configs:\n\u001b[0;32m    465\u001b[0m     use_legacy_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m layer_config\n",
      "File \u001b[1;32mg:\\Python311\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mg:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mg:\\Python311\\Lib\\site-packages\\keras\\src\\mixed_precision\\policy.py:360\u001b[0m, in \u001b[0;36mglobal_policy\u001b[1;34m()\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base_layer_utils\u001b[38;5;241m.\u001b[39mv2_dtype_behavior_enabled():\n\u001b[1;32m--> 360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Policy(\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloatx\u001b[49m())\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Policy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_infer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.src.backend' has no attribute 'floatx'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function.\n",
    "\"\"\"\n",
    "files = os.listdir(\"../../data/processed\")\n",
    "\n",
    "for file in files:\n",
    "    lstm_prediction(\"../../data/processed/\" + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
