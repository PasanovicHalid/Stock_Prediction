{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">LSTM Model for Stock Prediction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import of Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from re import X\n",
    "\n",
    "import keras;\n",
    "from kerastuner import HyperModel, RandomSearch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Functions for Loading preprocessed data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_from_csv(path: str):\n",
    "    \"\"\"\n",
    "    Imports data from a csv file and returns a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Functions for preparing data for LSTM model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_required_for_training(df: pd.DataFrame, features: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features required for training.\n",
    "    \"\"\"\n",
    "    return df[features]\n",
    "\n",
    "\n",
    "def split_data_into_training_and_test_sets(df: pd.DataFrame, window_start: int, window_end: int, test_size: float, columns: list[str]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Normalizes the entire data set, then splits it into training and test sets based on the given ratio.\n",
    "    \"\"\"\n",
    "    data_count = window_end - window_start\n",
    "    test_data_count = int(data_count * test_size)\n",
    "\n",
    "    end_of_training_data = window_start + test_data_count\n",
    "\n",
    "    df, max_values = normalize_data(df, columns, window_start, window_end)\n",
    "\n",
    "    return df.iloc[window_start : end_of_training_data], df.iloc[end_of_training_data : window_end], max_values\n",
    "\n",
    "\n",
    "def normalize_data(df: pd.DataFrame, columns: list[str], start: int, end: int):\n",
    "    \"\"\"\n",
    "    Normalizes the data in the given segment.\n",
    "    \"\"\"\n",
    "    max_values = {}\n",
    "    for column in columns:\n",
    "        max_value = df.iloc[start:end][column].max()\n",
    "        df.loc[start:end, column] = df.loc[start:end, column] / max_value\n",
    "        max_values[column] = max_value\n",
    "\n",
    "    return df, max_values\n",
    "\n",
    "\n",
    "def denormalize_data(data_array: np.ndarray, columns: list[str], max_values: dict):\n",
    "    \"\"\"\n",
    "    Denormalizes the data in the given segment.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        data_array[:] = data_array[:] * max_values[column]\n",
    "\n",
    "    return data_array\n",
    "\n",
    "\n",
    "def create_dataset(dataset: pd.DataFrame, time_step = 1) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Creates a dataset.\n",
    "    \"\"\"\n",
    "    dataX,dataY = [],[]\n",
    "\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        dataX.append(dataset.iloc[i:(i+time_step), 0])\n",
    "        dataY.append(dataset.iloc[i + time_step, 0])\n",
    "\n",
    "    return np.array(dataX),np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function for Building the LSTM model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds the LSTM model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(75, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(LSTM(75, return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(LSTM(75, return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(LSTM(75))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(512, activation=\"relu\")) \n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(1, activation=\"relu\"))\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"mae\", 'mape'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function for Hyperparameter optimization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=hp.Int('units', min_value=25, max_value=75, step=10), \n",
    "                    return_sequences=True, \n",
    "                    input_shape=self.input_shape))\n",
    "        model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        for i in range(hp.Int('layers', 1, 3)):\n",
    "            model.add(LSTM(units=hp.Int('units_' + str(i), min_value=25, max_value=75, step=10), \n",
    "                        return_sequences=True))\n",
    "            model.add(Dropout(hp.Float('dropout_' + str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(LSTM(units=hp.Int('units_final', min_value=32, max_value=512, step=32)))\n",
    "        model.add(Dropout(hp.Float('dropout_final', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        for i in range(hp.Int('dense_layers', 1, 3)):\n",
    "            model.add(Dense(hp.Int('dense_units_' + str(i), min_value=128, max_value=512, step=64), \n",
    "                            activation=hp.Choice('dense_activation_' + str(i), values=['relu', 'tanh', 'sigmoid'])))\n",
    "            model.add(Dropout(hp.Float('dense_dropout_' + str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(1, activation=\"relu\"))\n",
    "\n",
    "        model.compile(loss=\"mse\", \n",
    "                    optimizer=keras.optimizers.Adam(\n",
    "                        hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "                    metrics=[\"mae\", 'mape'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Functions for representing data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_on_whole_dataset(model, data, max_values, time_step, forecast_size):\n",
    "    \"\"\"\n",
    "    Forecasts the data.\n",
    "    \"\"\"\n",
    "    forecast = []\n",
    "    for i in range(forecast_size):\n",
    "        forecast.append(model.predict(data))\n",
    "        data = np.append(data, forecast[-1])\n",
    "        data = np.delete(data, 0)\n",
    "        data = data.reshape(1, time_step, 1)\n",
    "    forecast = np.array(forecast)\n",
    "    forecast = forecast.reshape(forecast_size, 1)\n",
    "    forecast = denormalize_data(forecast, [\"Close\"], max_values)\n",
    "    return forecast\n",
    "\n",
    "def plot_data(loaded_data, title: str, xlabel: str, ylabel: str, datacolumn: str):\n",
    "    \"\"\"\n",
    "    Plots the data from the dataframe.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.title(title)\n",
    "    plt.plot(loaded_data[datacolumn])\n",
    "    plt.xlabel(xlabel, fontsize=18)\n",
    "    plt.ylabel(ylabel, fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_data_list(loaded_data_list, title: str, xlabel: str, ylabel: str, datacolumns: list):\n",
    "    \"\"\"\n",
    "    Plots the data from the dataframe.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.title(title)\n",
    "    for loaded_data, datacolumn in zip(loaded_data_list, datacolumns):\n",
    "        plt.plot(loaded_data, label=datacolumn)\n",
    "    plt.xlabel(xlabel, fontsize=18)\n",
    "    plt.ylabel(ylabel, fontsize=18)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function for fiting the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_prediction(file: str):\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    file_name_without_extension = file_name.split(\".\")[0]\n",
    "\n",
    "    loaded_data = import_data_from_csv(file)\n",
    "    extracted_data = extract_features_required_for_training(loaded_data, [\"Adj Close\"])\n",
    "\n",
    "    time_step = 5\n",
    "    number_of_intervals = 5\n",
    "    percentage_of_test_interval_data = 0.8\n",
    "\n",
    "    interval_size = int(extracted_data.shape[0] / number_of_intervals)\n",
    "\n",
    "    X_training_data, Y_training_data = [], []\n",
    "    X_test_data, Y_test_data = [], []\n",
    "    max_values_list = []\n",
    "\n",
    "    for i in range(number_of_intervals):\n",
    "        start = i * interval_size\n",
    "        end = start + interval_size\n",
    "\n",
    "        if(i == number_of_intervals - 1):\n",
    "            end = extracted_data.shape[0]\n",
    "\n",
    "        training_data_interval, test_data_interval, max_values = split_data_into_training_and_test_sets(extracted_data, start, end, percentage_of_test_interval_data, [\"Adj Close\"])\n",
    "        \n",
    "        X_train, Y_train =  create_dataset(training_data_interval, time_step)\n",
    "        X_test, Y_test =  create_dataset(test_data_interval, time_step)\n",
    "\n",
    "        if(i != 0):\n",
    "            X_train = np.concatenate((X_test_data[i - 1], X_train))\n",
    "            Y_train = np.concatenate((Y_test_data[i - 1], Y_train))\n",
    "\n",
    "        X_training_data.append(X_train)\n",
    "        Y_training_data.append(Y_train)\n",
    "\n",
    "        X_test_data.append(X_test)\n",
    "        Y_test_data.append(Y_test)\n",
    "\n",
    "        max_values_list.append(max_values)\n",
    "\n",
    "    X_train_shape = (X_training_data[0].shape[1], 1)\n",
    "    #lstm_model = build_model(X_train_shape)\n",
    "\n",
    "    predictions_list = []\n",
    "    actuals_list = []\n",
    "\n",
    "    for i in range(number_of_intervals):\n",
    "        X_training_data_interval = X_training_data[i]\n",
    "        Y_training_data_interval = Y_training_data[i]\n",
    "        X_test_data_interval = X_test_data[i]\n",
    "        Y_test_data_interval = Y_test_data[i]\n",
    "        \n",
    "        file_path = \"checkpoints/\" + file_name_without_extension + \"/lstm-\" + str(i + 1) + \".keras\"\n",
    "\n",
    "        if(os.path.exists(file_path)):\n",
    "            lstm_model = load_model(file_path)\n",
    "            lstm_model.summary()\n",
    "\n",
    "        else:\n",
    "            file_path_previos = \"checkpoints/\" + file_name_without_extension + \"/lstm-\" + str(i) + \".keras\"\n",
    "\n",
    "            if(os.path.exists(file_path_previos)):\n",
    "                lstm_model = load_model(file_path_previos)\n",
    "\n",
    "            X_training_data_interval = np.reshape(X_training_data_interval, (X_training_data_interval.shape[0], X_training_data_interval.shape[1], 1))\n",
    "\n",
    "            ###Before hyperparameter tuning\n",
    "            #t_hist = lstm_model.fit(X_training_data_interval, Y_training_data_interval, batch_size = 15, epochs = 50)\n",
    "            \n",
    "            ###Hyperparameter tuning\n",
    "            hypermodel = LSTMHyperModel(input_shape=X_train_shape)\n",
    "            tuner = RandomSearch(\n",
    "                hypermodel,\n",
    "                objective='val_loss',\n",
    "                max_trials=20,\n",
    "                executions_per_trial=2,\n",
    "                directory='tuning_results',\n",
    "                project_name='lstm'+file_name_without_extension+str(i))\n",
    "\n",
    "            tuner.search(X_training_data_interval, Y_training_data_interval,\n",
    "                        epochs=20,\n",
    "                        validation_data=(X_test_data_interval, Y_test_data_interval))\n",
    "\n",
    "            best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "            lstm_model = tuner.hypermodel.build(best_hps)\n",
    "            t_hist = lstm_model.fit(X_training_data_interval, Y_training_data_interval, batch_size = 15, epochs = 50)\n",
    "            \n",
    "            pd.DataFrame(t_hist.history).plot(figsize=(8,5))\n",
    "            plt.show()\n",
    "\n",
    "            lstm_model.save(file_path)\n",
    "        \n",
    "        X_test_data_interval = np.reshape(X_test_data_interval, (X_test_data_interval.shape[0], X_test_data_interval.shape[1] ,1))\n",
    "        predictions = lstm_model.predict(X_test_data_interval)\n",
    "\n",
    "        denormalized_predictions = denormalize_data(predictions, [\"Adj Close\"], max_values_list[i])\n",
    "        denormalized_actuals = denormalize_data(Y_test_data_interval, [\"Adj Close\"], max_values_list[i])\n",
    "        plot_data_list([denormalized_predictions.flatten(), denormalized_actuals.flatten()], 'Predictions vs Actual for ' + str(i+1) + '. iteration of ' + file_name_without_extension, 'Date', 'Adj Close Price USD ($)', ['Predictions', 'Actual'])\n",
    "\n",
    "        rmse = np.sqrt(np.mean(((predictions - Y_test_data_interval) ** 2)))\n",
    "        print(\"Value of RSME for interval \" + str(i + 1) + \" is: \" + str(rmse))\n",
    "\n",
    "        predictions_list = np.concatenate((predictions_list, denormalized_predictions.flatten()))\n",
    "        actuals_list = np.concatenate((actuals_list, denormalized_actuals.flatten()))\n",
    "\n",
    "\n",
    "    plot_data_list([predictions_list, actuals_list], 'Predictions vs Actual for ' + file_name_without_extension, 'Date', 'Adj Close Price USD ($)', ['Predictions', 'Actual'])\n",
    "    print(\"NOTE: The big jumps in the values happen because the intervals have 80% of the training data between them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Main function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function.\n",
    "\"\"\"\n",
    "files = os.listdir(\"../../data/processed\")\n",
    "\n",
    "for file in files:\n",
    "    lstm_prediction(\"../../data/processed/\" + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
